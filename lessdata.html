<!DOCTYPE html>

<html lang="en">
<head>
    <meta name="generator" content=
    "HTML Tidy for Mac OS X (vers 31 October 2006 - Apple Inc. build 15.17), see www.w3.org">
    <meta charset="utf-8">
    <meta name="description" content="Tim Menzies">
    <meta name="author" content="{[author}}">

    <title>Tim Menzies: Many  Models Are Low-dimensional (or Not at All)</title>
    <link href='style.css' rel='stylesheet' type='text/css'>  

    <link rel="icon" href="img/favicon.png" type="image/png" />
    <link rel="stylesheet"
href="http://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css">
    
</head>
<!---
  <p><a href="http://facebook.com/"><i class="fa fa-github" style="color:#171516; font-size: 24px;"></i></a>
<a href="http://facebook.com/"><i class="fa fa-facebook-official" style="color:#3b5998; font-size: 24px;"></i></a>
<a href="http://twitter.com/"><i class="fa fa-twitter-square" style="color:#00aced; font-size: 24px;"></i></a>
<a href="http://facebook.com/"><i class="fa fa-slideshare" style="color:#007ee5; font-size: 24px;"></i></a>
<a href="http://facebook.com/"><i class="fa fa-dropbox" style="color:#007ee5; font-size: 24px;"></i></a>
<a href="http://linkedin.com/"><i class="fa fa-linkedin" style="color:#007bb6;; font-size: 24px;"></i></a></p><hr>
--->
<!-- ----------------------------------------------------------------- -->
<body>
<div class="wrapper">
  <header>
       
        <p class="alignleft">&nbsp;<a href="index.html">home</a> |
                                      <a href="bio.html">about me</a> | 
                                      <a href="contact.html">contact</a> 
        </p>
                                     <p
                                        class="alignright">
                                      <a href="https://scholar.google.com/citations?user=7htTUTgmLtUC&hl=en">papers</a> |
                                      <a href="books.html">books</a> | 
                                        <a href="http://ai4se.net">lab</a> |
                                      <a href="http://openscience.us/repo">data</a> |
                                      <a href="http://helenburgess.com">helen</a> &nbsp;&nbsp;
                                             </p><br clear=all>
           
            <h1>&nbsp; 
              <p class="alignleft">&nbsp;Tim Menzies</p><p
                                                           class="alignright">tim@menzies.us&nbsp;</p></h1><br clear=all>
        
       </header>
       <div id="core" class="clearfix">
         <section id="left">
	   

<h1>Many  Models Are Low-dimensional (or Not at All)</h1>



<p><em>The computing scientists main challenge is not to get confused by the complexities of (their) own making.</em> <br/>
-- E.W. Dijkstra</p>

<p>Much data is redundant, noisy, or irrelevant. For example:</p>

<ul>
    <li>If a model can be generated from a table of data, then that table contains enough examples to learn that model.</li>
    <li>That is, many rows are actually echoes of a smaller number of underlying effects called <em>prototypes</em>.</li>
</ul>

<p>So one way to use less data is to only share a small number of prototypes; i.e. use fewer rows.
Not only that, but we can use fewer columns:</p>

<ul>
    <li>The following figure 
    argues  in the expected case, models can only use a small number of columns.</li>
    <li>Otherwise, there would not be enough data to cover all the possible values of all the columns). </li>
</ul>

<p><a href="img/dist.jpg"><img src="img/dist.jpg" width=460></a></p>

<p> That is, if we were so foolish as to try to build high-dimensional models, we would fail as the region where we can find related examples would become vanishingly small. Note that this is often called the <em>curse of dimensionality</em>.</p>

<ul>
    <li><em>"When the dimensionality increases, the volume of the space increases so fast that the available data becomes sparse. This sparsity is problematic for any method that requires statistical significance. In order to obtain a statistically sound and reliable result, the amount of data needed to support the result often grows exponentially with the dimensionality."</em> -- <a href="http://en.wikipedia.org/wiki/Curse_of_dimensionality">Wikipedia</a></li>
</ul>


<p>Note this curse can also be a blessing:</p>

<ul>
    <li>Because it is impossible to find the data to support bigger models, then all we need ever do is build small ones;</li>
    <li>Which, in turn, means that we might be able to build those small models for just a little data;</li>
    <li>Which also means that we need only share small amounts of data.</li>
</ul>

<p>There is much empirical evidence that just because a data set has n columns, we need not use them all. Numerous researchers have examined what happens when a data miner deliberately ignores some of the columns in the training data. For example, the experiments of Ron Kohavi and George John show that, on numerous real-world datasets, over 80% of columns can be ignored. Further, ignoring those columns doesn't degrade the learner's classification accuracy (in fact, it sometimes even results in small improvements).</p>

<p>Further, if we combine both prototype and column selection, the net result can be a dramatic reduction in the complexity of the data:</p>

<ul>
    <li>In the book <a href="http://www.sciencedirect.com/science/article/pii/B9780124172951000151">Sharing Data and Models in Software Enginerring, chapter 15</a>,
    we show experiments where 100s to 1000s of defect reports to
    tiny tables 2 dozen rows and half a cdozen columns.</li>
    <li>A concern with making such a short summary is that some information has been lost. To check that, Papakroni
    build defect predictors (using Random Forests) from the full data and the condensed data.</li>
    <li>But, in those experiments,
    the predictions from the summary were no different than those using the full data set.</li>
</ul>

<p>The only way larger data sets can be summarized to smaller ones is
if there is some superfluous details in the larger set. Hence,
before we can advocate such summarizations we must first offer a
measure of data set simplicity and only summarize the simpler data.
The next figure offers <em>intrinsic dimensionality</em> as such a measure and
applies it to 10 data sets with 21 columns of data. As shown in
that figure, the intrinsic dimensionality of our data sets can be
very small indeed. It is hardly surprising that such an intrinsically
low-dimensional data set can be summaried in half a dozen columns
and a few dozen rows.</p>

<p><a href="img/dim.jpg"><img style="padding:7px;" border=1 src="img/dim.jpg" width=460></a></p>



	   </section>
         <section id="right">
           <center>
             <p>Professor.full.cs<br>AI + SE<br>NC State, USA</p>
    <small>

           <p><a href="https://www.facebook.com/timmenzies"><i class="fa fa-facebook-official" style="padding-right:5px;  color:#3b5998; font-size: 25px;"></i></a>
                                      <a href="http://github.com/timm"><i class="fa fa-github" style="padding-right:5px; color:#171516; font-size: 25px;"></i></a>
                                      <a href="https://twitter.com/timmenzies"><i class="fa fa-twitter-square" style="padding-right:5px; color:#00aced; font-size: 25px;"></i></a><br>
<a href="http://slideshare.com/timmenzies"><i class="fa fa-slideshare" style="padding-right:5px; color:#007ee5; font-size: 25px;"></i></a>
<a href="https://www.dropbox.com/sh/2hkf4q271jg5t0o/AAAp9ggE1iMC5bgORnseJay7a?dl=0"><i class="fa fa-dropbox" style="padding-right:5px; color:#007ee5; font-size: 25px;"></i></a>
<a href="https://www.linkedin.com/in/tim-menzies-444183"><i class="fa fa-linkedin" style="padding-right:5px; color:#007bb6;; font-size: 25px;"></i></a>

           </center>
           <hr>

           
<p id="class0"><b>Sept 9</b>:
Why DE is better than grid search. Sent to IST journal.
<a href="http://arxiv.org/abs/1609.02613">More...</a></p>
	    
<p id="class1"><b>Sept 7</b>:
My new book is here.
<a href="http://www.amazon.com/Perspectives-Data-Science-Software-Engineering/dp/0128042060">More...</a></p>
	    
<p id="class0"><b>Sept 1</b>:
I&#39;m now an author of a top 100 most cited&#x2F;year papers in SE
<a href="http://www.researchgate.net/publication/284563030_Highly-cited_papers_in_software_engineering_The_top-100">More...</a></p>
	    
<p id="class1"><b>Aug30</b>:
Paper accepted with minor revisions, EMSE journal, Negative Results about Effort Estimation
</p>
	    
<p id="class0"><b>Aug26</b>:
Breakthrough in multi-objective optimization Sent to TSE
<a href="http://arxiv.org/abs/1608.07617">More...</a></p>
	    
<p id="class1"><b>Aug26</b>:
9000+ SE conference papers clustered. Sent to ICSE&#39;17
<a href="http://arxiv.org/abs/1608.08100">More...</a></p>
	    
<p id="class0"><b>Aug26</b>:
How to fix topic modeling in SE. Sent to ICSE&#39;17
<a href="http://arxiv.org/abs/1608.08176">More...</a></p>
	    
<p id="class1"><b>Aug24</b>:
New gift, $60K, Lexis Nexis, Industrial Text Mining
</p>
	    
<p id="class0"><b>Aug15</b>:
New graont, $85K, Learning Analytics Sciences, Data sharing and privacy
</p>
	    
<p id="class1"><b>Aug13</b>:
Paper accepted with minor revisions, EMSE journal, Phase Delay and SE
</p>
	    
<p id="class0"><b>Jul 28</b>:
Invited to program committee ICSE;18
<a href="http://www.icse2018.org/committee/icse-2018-program-committee">More...</a></p>
	    
<p id="class1"><b>Jul 8</b>:
New appointment: editorial board Journal of Big Data research
<a href="http://digital-library.theiet.org/journals/iet-sen/editorial-board">More...</a></p>
	    
    </small>
    <a href="news.html"><img src="http://www.hcdistrictclerk.com/ChildSupport.Web/images/icons/icon_MoreNews.gif"></a>

     
            </section>            
    

        </div>
       <footer>
                 
         <center>

<hr>
                <p><a href="https://www.youtube.com/watch?v=lQvDmz4MQB4">
            <pre> ,-_|\    &quot;And I somehow fancy that I&#39;d like to change with Clancy,             
&#x2F;     \    Like to take a turn at droving where the seasons come and go,        
\_,-._*    While he faced the round eternal of grant writings and the journals -
     v     But I doubt he&#39;d like the coding, Clancy, of &#39;The Overflow&#39;.&quot;        </pre> 
      </a></p>
            <hr>
            <p>&copy; <a href="http://unlicense.org/">copyright</a> 2016, Tim Menzies. Share and enjoy.</p>
          </center>
        </footer>	
</div>
</body>
</html>


